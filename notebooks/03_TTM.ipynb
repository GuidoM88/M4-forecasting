{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "054c2abe-cb2f-46ab-a281-82509b9a1b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Libraries imported!\n",
      " Device: mps\n",
      "\n",
      "================================================================================\n",
      "CONFIGURATION\n",
      "================================================================================\n",
      "Context: 90 days | Horizon: 14 days\n",
      "Model: ibm-granite/granite-timeseries-ttm-r2 (90-30-ft-l1-r2.1)\n",
      "Fine-tune: 500 series | Test: 1000 series\n",
      "Epochs: 20 | Batch: 64 | LR: 0.001\n",
      "\n",
      "================================================================================\n",
      "LOADING DATA\n",
      "================================================================================\n",
      " Train: (4227, 9920) | Test: (4227, 15)\n",
      " Total series: 4227\n",
      "\n",
      " Preparing 500 series for fine-tuning...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a71e5573fa5e4590a2e0de36c1c31c9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PREPROCESSING\n",
      "================================================================================\n",
      "Before filtering:\n",
      "  Train: 315209 rows, 494 series\n",
      "  Valid: 145834 rows, 470 series\n",
      "  Test: 148799 rows, 494 series\n",
      "\n",
      "After filtering (min_length=106):\n",
      "  Train: 309703 rows, 429 series\n",
      "  Valid: 145806 rows, 458 series\n",
      "  Test: 148781 rows, 488 series\n",
      "Train: 53103 | Valid: 19726 | Test: 98517\n",
      "\n",
      "================================================================================\n",
      "LOADING MODEL\n",
      "================================================================================\n",
      "Model loaded! Parameters: 440,145\n",
      "\n",
      "================================================================================\n",
      "FINE-TUNING\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16600' max='16600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16600/16600 38:10, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.132700</td>\n",
       "      <td>0.146887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.131900</td>\n",
       "      <td>0.144521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.131800</td>\n",
       "      <td>0.145905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.131800</td>\n",
       "      <td>0.145095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.132100</td>\n",
       "      <td>0.143331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.132200</td>\n",
       "      <td>0.146785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.131200</td>\n",
       "      <td>0.144018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.130600</td>\n",
       "      <td>0.143132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.129700</td>\n",
       "      <td>0.146301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.128700</td>\n",
       "      <td>0.143252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.127900</td>\n",
       "      <td>0.141490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.126800</td>\n",
       "      <td>0.140856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.124800</td>\n",
       "      <td>0.138445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.123400</td>\n",
       "      <td>0.138600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.121800</td>\n",
       "      <td>0.138422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.120100</td>\n",
       "      <td>0.136659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.119200</td>\n",
       "      <td>0.136599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.118400</td>\n",
       "      <td>0.135770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.117900</td>\n",
       "      <td>0.135418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.117700</td>\n",
       "      <td>0.135486</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TrackingCallback] Mean Epoch Time = 104.02872111797333 seconds, Total Train Time = 2293.7512550354004\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='770' max='770' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [770/770 00:19]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Test Loss: 0.1498\n",
      "\n",
      "================================================================================\n",
      "FINAL EVALUATION\n",
      "================================================================================\n",
      "\n",
      " Evaluating 1000 series...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52786de2ff3d46e3a1669e2bdf583932",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Forecasting:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "RESULTS\n",
      "================================================================================\n",
      "\n",
      " Series evaluated: 1000\n",
      "\n",
      "   sMAPE (mean):   2.5115 ± 1.7323\n",
      "   sMAPE (median): 2.0262\n",
      "\n",
      "   MASE (mean):    2.7201 ± 1.9670\n",
      "   MASE (median):  2.2055\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # TTM on M4 Daily\n",
    "#\n",
    "# **Dataset**: M4 Daily (4,227 series, horizon=14 days)  \n",
    "# **Model**: IBM Granite TTM R2  \n",
    "\n",
    "# %% [markdown]\n",
    "# ## Imports\n",
    "\n",
    "# %%\n",
    "import math\n",
    "import os\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from torch.utils.data import Subset\n",
    "from transformers import EarlyStoppingCallback, Trainer, TrainingArguments, set_seed\n",
    "\n",
    "from tsfm_public import (\n",
    "    ForecastDFDataset,\n",
    "    TimeSeriesForecastingPipeline,\n",
    "    TimeSeriesPreprocessor,\n",
    "    TinyTimeMixerForPrediction,\n",
    "    TrackingCallback,\n",
    "    count_parameters,\n",
    ")\n",
    "from tsfm_public.toolkit.time_series_preprocessor import prepare_data_splits\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "set_seed(42)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\" Libraries imported!\")\n",
    "print(f\" Device: {device}\\n\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Configuration\n",
    "\n",
    "# %%\n",
    "# Model & Data\n",
    "FORECAST_LENGTH = 14\n",
    "CONTEXT_LENGTH = 90\n",
    "TTM_MODEL_PATH = \"ibm-granite/granite-timeseries-ttm-r2\"\n",
    "REVISION = \"90-30-ft-l1-r2.1\"\n",
    "\n",
    "# Paths\n",
    "data_path = Path('../data/M4')\n",
    "train_file = data_path / 'Daily-train.csv'\n",
    "test_file = data_path / 'Daily-test.csv'\n",
    "\n",
    "# Training\n",
    "N_SERIES_FINETUNE = 500   \n",
    "N_SERIES_TEST = 1000      \n",
    "FEWSHOT_FRACTION = 0.20   # 20% of data for training\n",
    "NUM_EPOCHS = 20             \n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"CONFIGURATION\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Context: {CONTEXT_LENGTH} days | Horizon: {FORECAST_LENGTH} days\")\n",
    "print(f\"Model: {TTM_MODEL_PATH} ({REVISION})\")\n",
    "print(f\"Fine-tune: {N_SERIES_FINETUNE} series | Test: {N_SERIES_TEST} series\")\n",
    "print(f\"Epochs: {NUM_EPOCHS} | Batch: {BATCH_SIZE} | LR: {LEARNING_RATE}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Load M4 Daily Data\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LOADING DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "train_df = pd.read_csv(train_file)\n",
    "test_df = pd.read_csv(test_file)\n",
    "\n",
    "print(f\" Train: {train_df.shape} | Test: {test_df.shape}\")\n",
    "print(f\" Total series: {len(train_df)}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Prepare Data for TTM\n",
    "\n",
    "# %%\n",
    "def prepare_m4_for_ttm_clean(train_df, test_df, series_indices, include_test=False):\n",
    "    \"\"\"\n",
    "    Convert M4 wide format to long format\n",
    "    \n",
    "    Args:\n",
    "        train_df: M4 train data\n",
    "        test_df: M4 test data (only used if include_test=True)\n",
    "        series_indices: which series to include\n",
    "        include_test: if True, concatenate test data (for evaluation only!)\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame in long format\n",
    "    \"\"\"\n",
    "    all_data = []\n",
    "    \n",
    "    for idx in tqdm(series_indices, desc=\"Converting\"):\n",
    "        series_id = train_df.iloc[idx, 0]\n",
    "        train_values = train_df.iloc[idx, 1:].dropna().values\n",
    "        \n",
    "        if include_test:\n",
    "            test_values = test_df.iloc[idx, 1:].dropna().values\n",
    "            all_values = np.concatenate([train_values, test_values])\n",
    "        else:\n",
    "            all_values = train_values\n",
    "        \n",
    "        if len(all_values) < CONTEXT_LENGTH + FORECAST_LENGTH:\n",
    "            continue\n",
    "        \n",
    "        timestamps = pd.date_range(start='2015-01-01', periods=len(all_values), freq='D')\n",
    "        \n",
    "        df_series = pd.DataFrame({\n",
    "            'timestamp': timestamps,\n",
    "            'series_id': series_id,\n",
    "            'value': all_values,\n",
    "        })\n",
    "        \n",
    "        all_data.append(df_series)\n",
    "    \n",
    "    return pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "print(f\"\\n Preparing {N_SERIES_FINETUNE} series for fine-tuning...\")\n",
    "\n",
    "finetune_indices = list(range(N_SERIES_FINETUNE))\n",
    "\n",
    "# fine-tuning!\n",
    "data_finetune = prepare_m4_for_ttm_clean(\n",
    "    train_df, \n",
    "    test_df, \n",
    "    finetune_indices, \n",
    "    include_test=False  \n",
    ")\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Train Preprocessor & Create Datasets\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PREPROCESSING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Column specifiers\n",
    "column_specifiers = {\n",
    "    \"timestamp_column\": \"timestamp\",\n",
    "    \"id_columns\": [\"series_id\"],\n",
    "    \"target_columns\": [\"value\"],\n",
    "}\n",
    "\n",
    "# Train preprocessor\n",
    "tsp = TimeSeriesPreprocessor(\n",
    "    **column_specifiers,\n",
    "    context_length=CONTEXT_LENGTH,\n",
    "    prediction_length=FORECAST_LENGTH,\n",
    "    scaling=True,\n",
    "    encode_categorical=True,\n",
    "    scaler_type=\"standard\",\n",
    ")\n",
    "\n",
    "# 80% of TRAIN data for scaler\n",
    "train_end_date = data_finetune['timestamp'].quantile(0.8)\n",
    "df_train = data_finetune[data_finetune['timestamp'] <= train_end_date]\n",
    "\n",
    "trained_tsp = tsp.train(df_train)\n",
    "\n",
    "# Prepare splits\n",
    "split_params = {\"train\": 0.6, \"test\": 0.2}\n",
    "train_data, valid_data, test_data = prepare_data_splits(\n",
    "    data_finetune,  \n",
    "    id_columns=column_specifiers[\"id_columns\"],\n",
    "    split_config=split_params,\n",
    "    context_length=CONTEXT_LENGTH\n",
    ")\n",
    "\n",
    "# Filter out series that are too short after splitting\n",
    "min_length = CONTEXT_LENGTH + FORECAST_LENGTH + 2  # +2 for safety\n",
    "\n",
    "def filter_short_series(df, id_cols, min_len):\n",
    "    \"\"\"Remove series with insufficient data\"\"\"\n",
    "    series_counts = df.groupby(id_cols).size()\n",
    "    valid_series = series_counts[series_counts >= min_len].index\n",
    "    return df[df[id_cols[0]].isin(valid_series)]\n",
    "\n",
    "print(f\"Before filtering:\")\n",
    "print(f\"  Train: {len(train_data)} rows, {train_data['series_id'].nunique()} series\")\n",
    "print(f\"  Valid: {len(valid_data)} rows, {valid_data['series_id'].nunique()} series\")\n",
    "print(f\"  Test: {len(test_data)} rows, {test_data['series_id'].nunique()} series\")\n",
    "\n",
    "train_data = filter_short_series(train_data, column_specifiers[\"id_columns\"], min_length)\n",
    "valid_data = filter_short_series(valid_data, column_specifiers[\"id_columns\"], min_length)\n",
    "test_data = filter_short_series(test_data, column_specifiers[\"id_columns\"], min_length)\n",
    "\n",
    "print(f\"\\nAfter filtering (min_length={min_length}):\")\n",
    "print(f\"  Train: {len(train_data)} rows, {train_data['series_id'].nunique()} series\")\n",
    "print(f\"  Valid: {len(valid_data)} rows, {valid_data['series_id'].nunique()} series\")\n",
    "print(f\"  Test: {len(test_data)} rows, {test_data['series_id'].nunique()} series\")\n",
    "\n",
    "# Create datasets\n",
    "frequency_token = tsp.get_frequency_token(tsp.freq)\n",
    "dataset_params = {\n",
    "    \"timestamp_column\": column_specifiers[\"timestamp_column\"],\n",
    "    \"id_columns\": column_specifiers[\"id_columns\"],\n",
    "    \"target_columns\": column_specifiers[\"target_columns\"],\n",
    "    \"frequency_token\": frequency_token,\n",
    "    \"context_length\": CONTEXT_LENGTH,\n",
    "    \"prediction_length\": FORECAST_LENGTH,\n",
    "}\n",
    "\n",
    "train_dataset = ForecastDFDataset(tsp.preprocess(train_data), **dataset_params)\n",
    "valid_dataset = ForecastDFDataset(tsp.preprocess(valid_data), **dataset_params)\n",
    "test_dataset = ForecastDFDataset(tsp.preprocess(test_data), **dataset_params)\n",
    "\n",
    "# Few-shot sampling\n",
    "n_train_all = len(train_dataset)\n",
    "train_index = np.random.permutation(n_train_all)[:int(FEWSHOT_FRACTION * n_train_all)]\n",
    "train_dataset = Subset(train_dataset, train_index)\n",
    "\n",
    "n_valid_all = len(valid_dataset)\n",
    "valid_index = np.random.permutation(n_valid_all)[:int(FEWSHOT_FRACTION * n_valid_all)]\n",
    "valid_dataset = Subset(valid_dataset, valid_index)\n",
    "\n",
    "print(f\"Train: {len(train_dataset)} | Valid: {len(valid_dataset)} | Test: {len(test_dataset)}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Load & Fine-tune TTM Model\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LOADING MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "finetune_forecast_model = TinyTimeMixerForPrediction.from_pretrained(\n",
    "    TTM_MODEL_PATH,\n",
    "    revision=REVISION,\n",
    "    context_length=CONTEXT_LENGTH,\n",
    "    prediction_filter_length=FORECAST_LENGTH,\n",
    "    num_input_channels=tsp.num_input_channels,\n",
    "    prediction_channel_indices=tsp.prediction_channel_indices,\n",
    ")\n",
    "\n",
    "print(f\"Model loaded! Parameters: {count_parameters(finetune_forecast_model):,}\")\n",
    "\n",
    "# Configure training\n",
    "OUT_DIR = \"../outputs/ttm_finetuned_clean/\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "finetune_forecast_args = TrainingArguments(\n",
    "    output_dir=os.path.join(OUT_DIR, \"output\"),\n",
    "    overwrite_output_dir=True,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    do_eval=True,\n",
    "    eval_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=2 * BATCH_SIZE,\n",
    "    dataloader_num_workers=1,\n",
    "    report_to=\"none\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    save_total_limit=1,\n",
    "    logging_dir=os.path.join(OUT_DIR, \"logs\"),\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    use_cpu=(device == \"cpu\"),\n",
    ")\n",
    "\n",
    "early_stopping_callback = EarlyStoppingCallback(\n",
    "    early_stopping_patience=10,\n",
    "    early_stopping_threshold=0.0,\n",
    ")\n",
    "tracking_callback = TrackingCallback()\n",
    "\n",
    "optimizer = AdamW(finetune_forecast_model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = OneCycleLR(\n",
    "    optimizer,\n",
    "    LEARNING_RATE,\n",
    "    epochs=NUM_EPOCHS,\n",
    "    steps_per_epoch=math.ceil(len(train_dataset) / BATCH_SIZE),\n",
    ")\n",
    "\n",
    "finetune_forecast_trainer = Trainer(\n",
    "    model=finetune_forecast_model,\n",
    "    args=finetune_forecast_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    callbacks=[early_stopping_callback, tracking_callback],\n",
    "    optimizers=(optimizer, scheduler),\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINE-TUNING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "finetune_forecast_trainer.train()\n",
    "\n",
    "test_results = finetune_forecast_trainer.evaluate(test_dataset)\n",
    "print(f\"\\n Test Loss: {test_results['eval_loss']:.4f}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Evaluation Functions\n",
    "\n",
    "# %%\n",
    "def smape(y_true, y_pred):\n",
    "    \"\"\"Symmetric Mean Absolute Percentage Error\"\"\"\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2.0\n",
    "    diff = np.abs(y_true - y_pred) / denominator\n",
    "    diff[denominator == 0] = 0.0\n",
    "    return 100 * np.mean(diff)\n",
    "\n",
    "def mase(y_true, y_pred, y_train, seasonality=1):\n",
    "    \"\"\"Mean Absolute Scaled Error\"\"\"\n",
    "    mae = np.mean(np.abs(y_true - y_pred))\n",
    "    naive_mae = np.mean(np.abs(y_train[seasonality:] - y_train[:-seasonality]))\n",
    "    if naive_mae == 0:\n",
    "        return np.nan\n",
    "    return mae / naive_mae\n",
    "\n",
    "def evaluate_ttm_m4(model, tsp, train_df, test_df, start_idx, n_series, \n",
    "                    context_length, forecast_length, device):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    \n",
    "    smape_scores = []\n",
    "    mase_scores = []\n",
    "    \n",
    "    end_idx = min(start_idx + n_series, len(train_df))\n",
    "    \n",
    "    print(f\"\\n Evaluating {end_idx - start_idx} series...\")\n",
    "    \n",
    "    # Get scaler stats\n",
    "    if hasattr(tsp, 'scaler') and tsp.scaler is not None:\n",
    "        scaler_mean = float(tsp.scaler.mean_[0])\n",
    "        scaler_scale = float(tsp.scaler.scale_[0])\n",
    "        use_scaler = True\n",
    "    else:\n",
    "        scaler_mean = 0.0\n",
    "        scaler_scale = 1.0\n",
    "        use_scaler = False\n",
    "    \n",
    "    # Get frequency token\n",
    "    freq_token = tsp.get_frequency_token(tsp.freq)\n",
    "    freq_token = torch.tensor([freq_token], dtype=torch.long, device=device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx in tqdm(range(start_idx, end_idx), desc=\"Forecasting\"):\n",
    "            try:\n",
    "                train_values = train_df.iloc[idx, 1:].dropna().values\n",
    "                test_values = test_df.iloc[idx, 1:].dropna().values\n",
    "                \n",
    "                if len(train_values) < context_length or len(test_values) < forecast_length:\n",
    "                    continue\n",
    "                \n",
    "                context = train_values[-context_length:].astype(np.float64)\n",
    "                \n",
    "                # Normalize\n",
    "                context_normalized = (context - scaler_mean) / scaler_scale if use_scaler else context\n",
    "                \n",
    "                # Create tensor\n",
    "                past_values = torch.from_numpy(\n",
    "                    context_normalized.astype(np.float32)\n",
    "                ).unsqueeze(0).unsqueeze(-1).to(device)\n",
    "                \n",
    "                # Forward\n",
    "                outputs = model(past_values=past_values, freq_token=freq_token)\n",
    "                \n",
    "                # Extract prediction\n",
    "                if hasattr(outputs, 'prediction_outputs'):\n",
    "                    forecast = outputs.prediction_outputs.squeeze().cpu().numpy()\n",
    "                elif hasattr(outputs, 'logits'):\n",
    "                    forecast = outputs.logits.squeeze().cpu().numpy()\n",
    "                else:\n",
    "                    forecast = outputs.squeeze().cpu().numpy()\n",
    "                \n",
    "                forecast = forecast[:forecast_length]\n",
    "                \n",
    "                # Denormalize\n",
    "                forecast_denorm = forecast * scaler_scale + scaler_mean if use_scaler else forecast\n",
    "                \n",
    "                # Compare with M4 test data\n",
    "                y_true = test_values[:forecast_length]\n",
    "                y_pred = forecast_denorm[:forecast_length]\n",
    "                \n",
    "                if len(y_pred) == forecast_length and len(y_true) == forecast_length:\n",
    "                    smape_score = smape(y_true, y_pred)\n",
    "                    mase_score = mase(y_true, y_pred, train_values, seasonality=1)\n",
    "                    \n",
    "                    if not np.isnan(smape_score) and not np.isinf(smape_score):\n",
    "                        smape_scores.append(smape_score)\n",
    "                    \n",
    "                    if not np.isnan(mase_score) and not np.isinf(mase_score):\n",
    "                        mase_scores.append(mase_score)\n",
    "                \n",
    "            except Exception as e:\n",
    "                continue\n",
    "    \n",
    "    return {\n",
    "        'smape_mean': np.mean(smape_scores) if smape_scores else np.nan,\n",
    "        'smape_median': np.median(smape_scores) if smape_scores else np.nan,\n",
    "        'smape_std': np.std(smape_scores) if smape_scores else np.nan,\n",
    "        'mase_mean': np.mean(mase_scores) if mase_scores else np.nan,\n",
    "        'mase_median': np.median(mase_scores) if mase_scores else np.nan,\n",
    "        'mase_std': np.std(mase_scores) if mase_scores else np.nan,\n",
    "        'n_series': len(smape_scores),\n",
    "        'smape_scores': smape_scores,\n",
    "        'mase_scores': mase_scores,\n",
    "    }\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Final Evaluation on Test Set\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL EVALUATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "results = evaluate_ttm_m4(\n",
    "    model=finetune_forecast_model,\n",
    "    tsp=trained_tsp,\n",
    "    train_df=train_df,\n",
    "    test_df=test_df,\n",
    "    start_idx=N_SERIES_FINETUNE,\n",
    "    n_series=N_SERIES_TEST,\n",
    "    context_length=CONTEXT_LENGTH,\n",
    "    forecast_length=FORECAST_LENGTH,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\n Series evaluated: {results['n_series']}\")\n",
    "print(f\"\\n   sMAPE (mean):   {results['smape_mean']:.4f} ± {results['smape_std']:.4f}\")\n",
    "print(f\"   sMAPE (median): {results['smape_median']:.4f}\")\n",
    "print(f\"\\n   MASE (mean):    {results['mase_mean']:.4f} ± {results['mase_std']:.4f}\")\n",
    "print(f\"   MASE (median):  {results['mase_median']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ed66cc-3414-4248-aca4-35ec40526878",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TTM new",
   "language": "python",
   "name": "ttm_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
